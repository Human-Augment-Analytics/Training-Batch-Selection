{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Training with Loss-Based Batch Selection\n",
    "\n",
    "This notebook demonstrates training BERT on IMDB with two strategies:\n",
    "1. **Normal**: Standard training on all examples\n",
    "2. **Loss-based**: Filter examples based on previous loss\n",
    "\n",
    "**Note:** For quick testing in Colab, we'll use a subset of data. Remove the subset limits for full training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets scipy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Training configuration\n",
    "MAX_LENGTH = 128  # Reduced for faster training in Colab\n",
    "BATCH_SIZE = 8    # Reduced for Colab GPU memory\n",
    "EPOCHS = 3\n",
    "N_RUNS = 1        # Set to 1 for quick demo (use 3+ for robust results)\n",
    "LEARNING_RATE = 2e-5\n",
    "LOSS_THRESHOLD = 0.5\n",
    "\n",
    "# For quick testing, use subset of data\n",
    "USE_SUBSET = True\n",
    "TRAIN_SUBSET_SIZE = 1000  # Use 1000 samples for quick demo\n",
    "TEST_SUBSET_SIZE = 500\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Max Length: {MAX_LENGTH}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Runs: {N_RUNS}\")\n",
    "print(f\"  Using Subset: {USE_SUBSET}\")\n",
    "if USE_SUBSET:\n",
    "    print(f\"  Train Subset: {TRAIN_SUBSET_SIZE}\")\n",
    "    print(f\"  Test Subset: {TEST_SUBSET_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"Wrapper for HuggingFace IMDB dataset with tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, split='train', max_length=128, subset_size=None):\n",
    "        print(f\"Loading {split} dataset...\")\n",
    "        self.dataset = load_dataset('stanfordnlp/imdb', split=split)\n",
    "        \n",
    "        # Use subset if specified\n",
    "        if subset_size is not None:\n",
    "            indices = list(range(min(subset_size, len(self.dataset))))\n",
    "            self.dataset = self.dataset.select(indices)\n",
    "            print(f\"Using subset of {len(self.dataset)} samples\")\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item['text']\n",
    "        label = item['label']\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_batch_sampler(dataset, batch_size, **kwargs):\n",
    "    \"\"\"Standard random batch sampling.\"\"\"\n",
    "    n = len(dataset)\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    for start in range(0, n, batch_size):\n",
    "        yield indices[start:start + batch_size].tolist()\n",
    "\n",
    "\n",
    "def loss_based_batch_sampler(dataset, batch_size, loss_history=None, threshold=LOSS_THRESHOLD, **kwargs):\n",
    "    \"\"\"\n",
    "    Sample batches from examples with loss above threshold.\n",
    "    First epoch: uses all examples.\n",
    "    Subsequent epochs: only examples with loss > threshold.\n",
    "\n",
    "    Always ensures at least batch_size examples are selected to prevent empty epochs.\n",
    "    \"\"\"\n",
    "    if loss_history is None or loss_history.sum() == 0:\n",
    "        # First epoch: use all examples\n",
    "        indices = np.arange(len(dataset))\n",
    "    else:\n",
    "        # Filter to examples with loss above threshold\n",
    "        indices = np.where(loss_history > threshold)[0]\n",
    "\n",
    "        # IMPORTANT: Always ensure minimum number of samples to prevent empty epochs\n",
    "        min_samples = batch_size\n",
    "        if len(indices) < min_samples:\n",
    "            # Not enough samples - pick top-k by loss instead\n",
    "            top_k = min(min_samples, len(dataset))\n",
    "            indices = np.argsort(loss_history)[-top_k:]  # Top-k highest loss samples\n",
    "            print(f\"  Filtering: Only {len(np.where(loss_history > threshold)[0])} examples above threshold, using top-{top_k} by loss instead\")\n",
    "        else:\n",
    "            print(f\"  Filtering: {len(indices)}/{len(dataset)} examples above threshold\")\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Yield batches, including any remaining samples in a final smaller batch\n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[start:start + batch_size]\n",
    "        yield batch_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, test_dataset, epochs, batch_size,\n",
    "                batch_sampler_fn, strategy_name, seed=None):\n",
    "    \"\"\"Train BERT model with specified batch sampling strategy.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Calculate total training steps for scheduler\n",
    "    n_samples = len(train_dataset)\n",
    "    steps_per_epoch = n_samples // batch_size\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Track per-sample loss\n",
    "    per_sample_loss = np.zeros(len(train_dataset))\n",
    "\n",
    "    # Metrics storage\n",
    "    train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "    samples_per_epoch = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct, n_samples_epoch, running_loss = 0, 0, 0\n",
    "\n",
    "        # Get batch iterator\n",
    "        if 'loss_history' in batch_sampler_fn.__code__.co_varnames:\n",
    "            batch_iter = batch_sampler_fn(train_dataset, batch_size, loss_history=per_sample_loss)\n",
    "        else:\n",
    "            batch_iter = batch_sampler_fn(train_dataset, batch_size)\n",
    "\n",
    "        # Training loop\n",
    "        for batch_indices in batch_iter:\n",
    "            # Prepare batch\n",
    "            batch = [train_dataset[i] for i in batch_indices]\n",
    "            input_ids = torch.stack([item['input_ids'] for item in batch]).to(DEVICE)\n",
    "            attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(DEVICE)\n",
    "            labels = torch.stack([item['label'] for item in batch]).to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            # Compute loss per sample\n",
    "            logits = outputs.logits\n",
    "            loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "            losses_per_sample = loss_fn(logits, labels)\n",
    "            loss = losses_per_sample.mean()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update metrics\n",
    "            running_loss += loss.item() * len(batch_indices)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            n_samples_epoch += len(batch_indices)\n",
    "\n",
    "            # Update per-sample loss history\n",
    "            if 'loss_history' in batch_sampler_fn.__code__.co_varnames:\n",
    "                for k, idx in enumerate(batch_indices):\n",
    "                    per_sample_loss[idx] = losses_per_sample[k].item()\n",
    "\n",
    "        # Epoch metrics\n",
    "        train_acc = correct / n_samples_epoch if n_samples_epoch > 0 else 0\n",
    "        train_loss = running_loss / n_samples_epoch if n_samples_epoch > 0 else 0\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        samples_per_epoch.append(n_samples_epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        test_acc, test_loss = evaluate(model, test_dataset, batch_size)\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"[{strategy_name}] Epoch {epoch+1}/{epochs}: \"\n",
    "              f\"train_acc={train_acc:.4f}, test_acc={test_acc:.4f}, \"\n",
    "              f\"train_loss={train_loss:.4f}, samples={n_samples_epoch}\")\n",
    "\n",
    "    return {\n",
    "        'train_acc': train_accs,\n",
    "        'test_acc': test_accs,\n",
    "        'train_loss': train_losses,\n",
    "        'test_loss': test_losses,\n",
    "        'samples_per_epoch': samples_per_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, batch_size):\n",
    "    \"\"\"Evaluate model on dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct, total, total_loss = 0, 0, 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[start:start + batch_size]\n",
    "            batch = [dataset[i] for i in batch_indices]\n",
    "\n",
    "            input_ids = torch.stack([item['input_ids'] for item in batch]).to(DEVICE)\n",
    "            attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(DEVICE)\n",
    "            labels = torch.stack([item['label'] for item in batch]).to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "\n",
    "            total_loss += loss.item() * len(batch_indices)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(batch_indices)\n",
    "\n",
    "    return correct / total, total_loss / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = IMDBDataset(\n",
    "    split='train',\n",
    "    max_length=MAX_LENGTH,\n",
    "    subset_size=TRAIN_SUBSET_SIZE if USE_SUBSET else None\n",
    ")\n",
    "\n",
    "test_dataset = IMDBDataset(\n",
    "    split='test',\n",
    "    max_length=MAX_LENGTH,\n",
    "    subset_size=TEST_SUBSET_SIZE if USE_SUBSET else None\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Experiments\n",
    "\n",
    "### Strategy 1: Normal (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STRATEGY 1: NORMAL (BASELINE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create fresh model\n",
    "model_normal = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "results_normal = train_model(\n",
    "    model_normal, train_dataset, test_dataset,\n",
    "    EPOCHS, BATCH_SIZE,\n",
    "    normal_batch_sampler, \"Normal\",\n",
    "    seed=42\n",
    ")\n",
    "time_normal = time.time() - start_time\n",
    "\n",
    "print(f\"\\nNormal strategy completed in {time_normal:.2f}s\")\n",
    "print(f\"Final test accuracy: {results_normal['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Loss-Based Batch Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY 2: LOSS-BASED BATCH SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create fresh model\n",
    "model_loss_based = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "results_loss_based = train_model(\n",
    "    model_loss_based, train_dataset, test_dataset,\n",
    "    EPOCHS, BATCH_SIZE,\n",
    "    loss_based_batch_sampler, \"Loss-Based\",\n",
    "    seed=42\n",
    ")\n",
    "time_loss_based = time.time() - start_time\n",
    "\n",
    "print(f\"\\nLoss-based strategy completed in {time_loss_based:.2f}s\")\n",
    "print(f\"Final test accuracy: {results_loss_based['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "epochs_range = np.arange(1, EPOCHS + 1)\n",
    "\n",
    "# Test Accuracy\n",
    "axes[0, 0].plot(epochs_range, results_normal['test_acc'], 'o-', label='Normal', linewidth=2)\n",
    "axes[0, 0].plot(epochs_range, results_loss_based['test_acc'], 's-', label='Loss-Based', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Test Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Train Loss\n",
    "axes[0, 1].plot(epochs_range, results_normal['train_loss'], 'o-', label='Normal', linewidth=2)\n",
    "axes[0, 1].plot(epochs_range, results_loss_based['train_loss'], 's-', label='Loss-Based', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Train Loss')\n",
    "axes[0, 1].set_title('Train Loss Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Train Accuracy\n",
    "axes[1, 0].plot(epochs_range, results_normal['train_acc'], 'o-', label='Normal', linewidth=2)\n",
    "axes[1, 0].plot(epochs_range, results_loss_based['train_acc'], 's-', label='Loss-Based', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Train Accuracy')\n",
    "axes[1, 0].set_title('Train Accuracy Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Samples per Epoch\n",
    "axes[1, 1].plot(epochs_range, results_normal['samples_per_epoch'], 'o-', label='Normal', linewidth=2)\n",
    "axes[1, 1].plot(epochs_range, results_loss_based['samples_per_epoch'], 's-', label='Loss-Based', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Number of Samples')\n",
    "axes[1, 1].set_title('Samples per Epoch')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nNormal Strategy:\")\n",
    "print(f\"  Final Test Accuracy: {results_normal['test_acc'][-1]:.4f}\")\n",
    "print(f\"  Training Time: {time_normal:.2f}s\")\n",
    "print(f\"  Samples per epoch: {results_normal['samples_per_epoch']}\")\n",
    "\n",
    "print(f\"\\nLoss-Based Strategy:\")\n",
    "print(f\"  Final Test Accuracy: {results_loss_based['test_acc'][-1]:.4f}\")\n",
    "print(f\"  Training Time: {time_loss_based:.2f}s\")\n",
    "print(f\"  Samples per epoch: {results_loss_based['samples_per_epoch']}\")\n",
    "\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"  - Loss-based filtering reduces samples in later epochs\")\n",
    "print(f\"  - Potential speedup: {(time_normal/time_loss_based):.2f}x\")\n",
    "print(f\"  - Accuracy difference: {(results_loss_based['test_acc'][-1] - results_normal['test_acc'][-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. (Optional) Test on Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both models on a few examples\n",
    "def predict_sentiment(model, text, tokenizer):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids = encoding['input_ids'].to(DEVICE)\n",
    "        attention_mask = encoding['attention_mask'].to(DEVICE)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        prediction = outputs.logits.argmax(dim=1).item()\n",
    "    \n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Terrible film. Complete waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\"\n",
    "]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"=\"*60)\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    pred_normal = predict_sentiment(model_normal, text, tokenizer)\n",
    "    pred_loss = predict_sentiment(model_loss_based, text, tokenizer)\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Text: {text}\")\n",
    "    print(f\"  Normal Model: {pred_normal}\")\n",
    "    print(f\"  Loss-Based Model: {pred_loss}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

# Enhanced Dataset Loader Configuration
# Fully config-driven - add datasets without touching Python code!

# ============================================================================
# Path Configuration
# ============================================================================
paths:
  datasets_root: "datasets"
  vision_subdir: "vision"
  nlp_subdir: "nlp"

# ============================================================================
# Download Method Definitions
# ============================================================================
download_methods:
  torchvision:
    description: "Download via torchvision.datasets"
    supported_formats: ["class_folders", "raw"]

  url:
    description: "Download from direct URL"
    supported_archives: ["tar.gz", "zip", "tar", "tgz"]

  huggingface:
    description: "Download from HuggingFace datasets"
    supported_formats: ["jsonl", "csv", "parquet"]

  kaggle:
    description: "Download from Kaggle datasets"
    requires: "Kaggle API credentials (~/.kaggle/kaggle.json)"
    supported_archives: ["zip"]

  manual:
    description: "Manual download required"

# ============================================================================
# Export Format Definitions
# ============================================================================
export_formats:
  class_folders:
    description: "Organize images into folders by class"
    structure: "{split}/{class}/{index:05d}.{ext}"

  raw:
    description: "Keep original torchvision format"

  jsonl:
    description: "Export as JSON Lines format"
    extension: ".jsonl"

  csv:
    description: "Export as CSV format"
    extension: ".csv"

# ============================================================================
# Vision Datasets
# ============================================================================
vision_datasets:

  # ===== CIFAR-10 =====
  cifar10:
    name: "cifar10"
    description: "CIFAR-10: 60k 32x32 color images in 10 classes"
    estimated_size_mb: 577

    download:
      method: "torchvision"
      class_name: "CIFAR10"
      splits:
        train:
          param_name: "train"
          param_value: true
        test:
          param_name: "train"
          param_value: false

    export:
      format: "class_folders"
      image_extension: "png"
      get_classes: "dataset.classes"  # Python expression to get class names
      get_label: "label"  # How to extract label from dataset item
      get_image: "image"  # How to extract image from dataset item

    validation:
      check_dirs: ["cifar-10-batches-py"]
      check_files: []

  # ===== CIFAR-100 =====
  cifar100:
    name: "cifar100"
    description: "CIFAR-100: 60k 32x32 color images in 100 classes"
    estimated_size_mb: 576

    download:
      method: "torchvision"
      class_name: "CIFAR100"
      splits:
        train:
          param_name: "train"
          param_value: true
        test:
          param_name: "train"
          param_value: false

    export:
      format: "class_folders"
      image_extension: "png"
      get_classes: "dataset.classes"
      get_label: "label"
      get_image: "image"

    validation:
      check_dirs: ["cifar-100-python"]

  # ===== MNIST =====
  mnist:
    name: "mnist"
    description: "MNIST: 70k 28x28 grayscale handwritten digits"
    estimated_size_mb: 340

    download:
      method: "torchvision"
      class_name: "MNIST"
      splits:
        train:
          param_name: "train"
          param_value: true
        test:
          param_name: "train"
          param_value: false

    export:
      format: "class_folders"
      image_extension: "png"
      num_classes: 10
      get_label: "label"
      get_image: "image"

    validation:
      check_dirs: ["MNIST"]

  # ===== QMNIST =====
  qmnist:
    name: "qmnist"
    description: "QMNIST: Extended MNIST with additional test sets"
    estimated_size_mb: 586

    download:
      method: "torchvision"
      class_name: "QMNIST"
      splits:
        train:
          param_name: "what"
          param_value: "train"
        test:
          param_name: "what"
          param_value: "test"

    export:
      format: "class_folders"
      image_extension: "png"
      num_classes: 10
      get_label: "label"
      get_image: "image"

    validation:
      check_dirs: ["QMNIST"]

  # ===== SVHN =====
  svhn:
    name: "svhn"
    description: "SVHN: Street View House Numbers dataset"
    estimated_size_mb: 4000

    download:
      method: "torchvision"
      class_name: "SVHN"
      splits:
        train:
          param_name: "split"
          param_value: "train"
        test:
          param_name: "split"
          param_value: "test"

    export:
      format: "raw"

    validation:
      check_files: ["train_32x32.mat", "test_32x32.mat"]

  # ===== CINIC-10 =====
  cinic-10:
    name: "cinic-10"
    description: "CINIC-10: CIFAR-10 extended with ImageNet images"
    estimated_size_mb: 1200

    download:
      method: "url"
      url: "https://datashare.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz"
      archive:
        format: "tar.gz"
        extract_to: "."
        flatten_nested_dir: "CINIC-10"
        remove_after_extract: true

    export:
      format: "raw"  # Already in class_folders format

    validation:
      check_dirs: ["train", "valid", "test"]

  # ===== Tiny ImageNet =====
  tiny-imagenet-200:
    name: "tiny-imagenet-200"
    description: "Tiny ImageNet: 200 classes, 500 images each"
    estimated_size_mb: 481

    download:
      method: "url"
      url: "http://cs231n.stanford.edu/tiny-imagenet-200.zip"
      archive:
        format: "zip"
        extract_to: "."
        flatten_nested_dir: "tiny-imagenet-200"
        remove_after_extract: true

    export:
      format: "raw"

    validation:
      check_dirs: ["train", "val", "test"]

  # ===== VOC 2012 =====
  voc2012:
    name: "voc2012"
    description: "PASCAL VOC 2012: Object detection and segmentation"
    estimated_size_mb: 3700

    download:
      method: "huggingface"
      dataset: "nateraw/pascal-voc-2012"
      splits: ["train", "val"]
      save_images: true

    export:
      format: "parquet"

    validation:
      check_dirs: ["train", "val"]

  # ===== Wikipedia =====
  wikipedia_dataset:
    name: "wikipedia_dataset"
    description: "Wikipedia: English articles dataset"
    estimated_size_mb: 69

    download:
      method: "huggingface"
      dataset: "wikimedia/wikipedia"
      subset: "20231101.en"
      splits: ["train"]

    export:
      format: "jsonl"
      filename_template: "{split}.jsonl"

    validation:
      check_dirs: ["texts"]
      check_files: ["categories.list", "wiki_top10cats.tar.gz"]

  # ===== Clothing-1M =====
  clothing-1m:
    name: "clothing-1m"
    description: "Clothing-1M: Large-scale noisy clothing dataset (NPZ format)"
    estimated_size_mb: 12000

    download:
      method: "manual"
      kaggle_alternative: "tsavumoon/clothing-1m"
      instructions: |
        Clothing-1M download options:

        OPTION 1 - Manual Download (Recommended):
          1. Visit: https://www.kaggle.com/datasets/tsavumoon/clothing-1m
          2. Click "Download" button (accept terms if needed)
          3. Move downloaded file: mv ~/Downloads/archive.zip {dataset_root}/
          4. Extract: cd {dataset_root} && unzip archive.zip

        OPTION 2 - Kaggle CLI (may be slow/stuck):
          1. Ensure Kaggle API is set up
          2. Run: kaggle datasets download -d tsavumoon/clothing-1m -p {dataset_root}
          3. Extract: cd {dataset_root} && unzip clothing-1m.zip

        Expected file after extraction: clothing1m.npz (~12GB)

    validation:
      check_files: ["clothing1m.npz"]

  # ===== ImageNet 2012 =====
  imagenet-2012:
    name: "imagenet-2012"
    description: "ImageNet 2012: 1.2M images, 1000 classes"
    estimated_size_mb: 169000

    download:
      method: "manual"
      instructions: |
        ImageNet 2012 requires manual setup:

        Option 1: Download from https://image-net.org/download.php
          - Requires account and license agreement

        Option 2: Use Kaggle
          - https://www.kaggle.com/competitions/imagenet-object-localization-challenge
          - Accept competition terms

        After downloading:
          - Place train/ and val/ directories in: {dataset_root}/

    validation:
      check_dirs: ["train", "val"]

# ============================================================================
# NLP Datasets
# ============================================================================
nlp_datasets:

  # ===== CoLA =====
  cola:
    name: "cola"
    description: "CoLA: Corpus of Linguistic Acceptability"
    estimated_size_mb: 5

    download:
      method: "huggingface"
      dataset: "glue"
      subset: "cola"
      splits: ["train", "validation", "test"]

    export:
      format: "jsonl"
      filename_template: "{split}.jsonl"

    validation:
      check_files: ["train.jsonl", "validation.jsonl", "test.jsonl"]

  # ===== SST-2 =====
  sst2:
    name: "sst2"
    description: "SST-2: Stanford Sentiment Treebank"
    estimated_size_mb: 10

    download:
      method: "huggingface"
      dataset: "glue"
      subset: "sst2"
      splits: ["train", "validation", "test"]

    export:
      format: "jsonl"
      filename_template: "{split}.jsonl"

    validation:
      check_files: ["train.jsonl", "validation.jsonl", "test.jsonl"]

  # ===== E2E NLG =====
  e2e_nlg:
    name: "e2e-dataset"
    description: "E2E NLG: End-to-End NLG Challenge dataset"
    estimated_size_mb: 15

    download:
      method: "url"
      sources:
        train:
          url: "https://github.com/tuetschek/e2e-dataset/raw/master/trainset.csv"
          output: "train.csv"
        validation:
          url: "https://github.com/tuetschek/e2e-dataset/raw/master/devset.csv"
          output: "validation.csv"
        test:
          url: "https://github.com/tuetschek/e2e-dataset/raw/master/testset_w_refs.csv"
          output: "test.csv"

    export:
      format: "csv_to_jsonl"
      convert_csv_to_jsonl: true
      lowercase_keys: true
      add_index: true
      remove_csv_after: true

    validation:
      check_files: ["train.csv", "validation.csv", "test.csv"]

# ============================================================================
# Display Configuration
# ============================================================================
display:
  header_char: "="
  subheader_char: "-"
  status_present: "‚úîÔ∏è"
  status_missing: "‚ùå"
  color_present: "\033[92m"
  color_missing: "\033[91m"
  color_reset: "\033[0m"
  icons:
    vision: "üìä"
    nlp: "üìä"
    download: "üîΩ"
    success: "‚úÖ"
    error: "‚ùå"
    skip: "‚è≠Ô∏è"
    warning: "‚ö†Ô∏è"
    info: "‚ÑπÔ∏è"

# ============================================================================
# Download Settings
# ============================================================================
download_settings:
  default_timeout: 300
  retry_attempts: 3
  chunk_size: 8192
  verify_ssl: true
  show_progress: true

# ============================================================================
# CLI Defaults
# ============================================================================
cli_defaults:
  auto_confirm: false
  force_download: false
  verbose: true
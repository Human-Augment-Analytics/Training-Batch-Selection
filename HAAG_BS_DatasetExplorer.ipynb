{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787f5ea9",
   "metadata": {},
   "source": [
    "# DATASET EXPLORER: Batch Selection Project\n",
    "\n",
    "Author: Laura Tomokiyo\n",
    "AI Assistance: ChatGPT\n",
    "\n",
    "This notebook illustrates how to load and inspect a number of common ML benchmark datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ed4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "SAVE_DATA_SAMPLES=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving data to a static directory for further inspection/display (optional)\n",
    "# SAVE_DATA_SAMPLES=True\n",
    "\n",
    "if SAVE_DATA_SAMPLES:\n",
    "    SAVEDIR='static_samples' \n",
    "    from torchvision.transforms.functional import to_pil_image\n",
    "    from torchvision.utils import save_image\n",
    "    safe = lambda s: re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75029e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: some environment printouts\n",
    "# possibly helpful in getting compatible versions.\n",
    "# I used numpy 1.26.4, torchvision 0.23.0+cu128, pillow 9.5.0\n",
    "\n",
    "import sys\n",
    "print('---')\n",
    "print(\"path:\", sys.path)\n",
    "print('---')\n",
    "print(\"numpy location:\", np.__file__)\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"python executable:\", sys.executable)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"pillow version:\", PIL.__version__)\n",
    "print(PIL.__file__)\n",
    "print(\"pandas:\", pd.__version__, pd.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared data directory\n",
    "DATASETS_DIR = \"/storage/ice-shared/cs8903onl/lw-batch-selection/datasets\"\n",
    "\n",
    "# Subdirectories for each dataset\n",
    "# Vision datasets\n",
    "CIFAR100_DIR   = os.path.join(DATASETS_DIR, \"vision/cifar100\")\n",
    "CIFAR10_DIR    = os.path.join(DATASETS_DIR, \"vision/cifar10\")\n",
    "MNIST_DIR      = os.path.join(DATASETS_DIR, \"vision/MNIST\")\n",
    "QMNIST_DIR     = os.path.join(DATASETS_DIR, \"vision/qmnist\")\n",
    "SVHN_DIR       = os.path.join(DATASETS_DIR, \"vision/svhn\")\n",
    "SVHN_FULL_DIR  = os.path.join(DATASETS_DIR, \"vision/svhn-full\")\n",
    "IMAGENET_DIR   = os.path.join(DATASETS_DIR, \"vision/imagenet-2012\")\n",
    "TINY_IMAGENET_DIR = os.path.join(DATASETS_DIR, \"vision/tiny-imagenet-200\")\n",
    "CLOTHING1M_DIR = os.path.join(DATASETS_DIR, \"vision/clothing-1m\")\n",
    "VOC2012_DIR    = os.path.join(DATASETS_DIR, \"vision/voc2012\")\n",
    "CINIC10_DIR    = os.path.join(DATASETS_DIR, \"vision/CINIC-10\")\n",
    "WIKI_DIR     = os.path.join(DATASETS_DIR, \"vision/wikipedia_dataset\")\n",
    "\n",
    "\n",
    "# NLP datasets\n",
    "COLA_DIR = os.path.join(DATASETS_DIR, \"nlp/cola\")\n",
    "SST2_DIR = os.path.join(DATASETS_DIR, \"nlp/sst2\")\n",
    "E2ENLG_DIR     = os.path.join(DATASETS_DIR, \"nlp/e2e-dataset\")\n",
    "\n",
    "\n",
    "# Check\n",
    "print(\"CIFAR100_DIR =\", CIFAR100_DIR)\n",
    "print(\"VOC2012_DIR  =\", VOC2012_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361cc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "\n",
    "def show_dataset_stats(dataset, name=\"Dataset\", is_nlp=False):\n",
    "    print(f\"üìä {name}\")\n",
    "    print(\"-\" * (len(name) + 4))\n",
    "    if is_nlp and hasattr(dataset, \"keys\"):\n",
    "        for split in dataset.keys():\n",
    "            print(f\"{split}: {len(dataset[split])} samples\")\n",
    "    else:\n",
    "        print(f\"Number of samples: {len(dataset)}\")\n",
    "        if hasattr(dataset, \"classes\"):\n",
    "            print(f\"Number of classes: {len(dataset.classes)}\")\n",
    "            print(f\"Example classes: {dataset.classes[:10]}\")\n",
    "    print()\n",
    "\n",
    "def show_samples(dataset, num_samples=5, is_nlp=False, save_samples=False, savedir='static_samples'):\n",
    "    if is_nlp and hasattr(dataset, \"keys\"):\n",
    "        split = \"train\" if \"train\" in dataset else list(dataset.keys())[0]\n",
    "        rows = []\n",
    "        for i in random.sample(range(len(dataset[split])), k=num_samples):\n",
    "            rows.append({k: (v.item() if hasattr(v, \"item\") else v) \n",
    "                         for k, v in dataset[split][i].items()})\n",
    "        display(pd.DataFrame(rows))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, num_samples, figsize=(2*num_samples, 2))\n",
    "        for i in range(num_samples):\n",
    "            img, label = dataset[i]\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img_np = img.permute(1, 2, 0).numpy()\n",
    "            else:\n",
    "                img_np = np.array(img)\n",
    "            axes[i].imshow(img_np.squeeze(), cmap=\"gray\" if img_np.ndim == 2 else None)\n",
    "            axes[i].set_title(getattr(dataset, \"classes\", [])[label] if hasattr(dataset, \"classes\") else label)\n",
    "            axes[i].axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "def show_split_sizes_pt(name, train=None, val=None, test=None, classes=None):\n",
    "    print(f\"üìä {name}\")\n",
    "    print(\"-\" * (len(name) + 4))\n",
    "    if train is not None: print(f\"Training set samples: {len(train)}\")\n",
    "    if val   is not None: print(f\"Validation set samples: {len(val)}\")\n",
    "    if test  is not None: print(f\"Test set samples: {len(test)}\")\n",
    "    if classes is None and train is not None and hasattr(train, \"classes\"):\n",
    "        classes = train.classes\n",
    "    if classes is not None:\n",
    "        ex = list(classes)[:10]\n",
    "        print(f\"Number of classes: {len(classes)}\")\n",
    "        print(f\"Example classes: {ex}\")\n",
    "    print()\n",
    "    \n",
    "def show_split_sizes_hf(ds_dict, name, label_field=None, show_example_labels=True):\n",
    "    print(f\"üìä {name}\")\n",
    "    print(\"-\" * (len(name) + 4))\n",
    "    for split in (\"train\", \"validation\", \"test\"):\n",
    "        if split in ds_dict:\n",
    "            print(f\"{split}: {len(ds_dict[split])} samples\")\n",
    "    # Try to show label examples if requested\n",
    "    if show_example_labels and \"train\" in ds_dict:\n",
    "        ex = ds_dict[\"train\"][0]\n",
    "        # Heuristics to find label field if not provided\n",
    "        if label_field is None:\n",
    "            for k in (\"label\", \"labels\", \"category\"):\n",
    "                if k in ex:\n",
    "                    label_field = k\n",
    "                    break\n",
    "        if label_field and label_field in ex:\n",
    "            print(f\"example label field: '{label_field}' (first example value: {ex[label_field]})\")\n",
    "    print()\n",
    "    \n",
    "# HuggingFace datasets return dicts ‚Üí wrap them in a PyTorch Dataset to enable DataLoader\n",
    "def hf_to_torch(dataset, text_field=\"sentence\", label_field=\"label\"):\n",
    "    return [(ex[text_field], torch.tensor(ex[label_field])) for ex in dataset]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68948b57",
   "metadata": {},
   "source": [
    "## Vision Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03c2fc",
   "metadata": {},
   "source": [
    "### CIFAR\n",
    "CIFAR-10, CIFAR-100: train/test \\\n",
    "CIFAR-100-LT: training set derived from CIFAR-100 \\\n",
    "Source: torchvision.datasets \\\n",
    "Cite: CIFAR-100, CIFAR-10: Krizhevsky, A. and Hinton, G. [Learning multiple layers of\n",
    "features from tiny images. Master‚Äôs thesis](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Department of Computer Science, University of Toronto, 2009\\\n",
    "Cite CIFAR-LT: Cui, Yin, et al. \"[Class-balanced loss based on effective number of samples](https://arxiv.org/pdf/1901.05555).\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211f2a0",
   "metadata": {},
   "source": [
    "#### CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ffbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect dataset \n",
    "\n",
    "cifar100_train = datasets.CIFAR100(root=CIFAR100_DIR, train=True, download=False, transform=transform)\n",
    "cifar100_test  = datasets.CIFAR100(root=CIFAR100_DIR, train=False, download=False, transform=transform)\n",
    "\n",
    "show_dataset_stats(cifar100_train, name=\"CIFAR-100 (train)\")\n",
    "show_dataset_stats(cifar100_test, name=\"CIFAR-100 (test)\")\n",
    "\n",
    "# Show samples explicitly\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = cifar100_train[i]\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(cifar100_train.classes[label])\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'cifar100')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "        # save each sample next to the notebook, e.g., static_samples/CIFAR100_train/000_dog.png\n",
    "        fn = os.path.join(OUTDIR,f\"{i:03d}_{cifar100_train.classes[label]}.png\")\n",
    "        print(f'saving {fn}')\n",
    "        save_image(img,fn)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(cifar100_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df00879",
   "metadata": {},
   "source": [
    "#### CIFAR-100-LT\n",
    "This \"long-tail\" subset is dynamically generated to be imbalanced. It seems that while the training subset is purposefully balanced in these experiments, the test subset is kept the same (i.e. balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3043a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# === Parameters ===\n",
    "IR = 100  # imbalance ratio (max/min class frequency)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# === Load CIFAR-100 train ===\n",
    "cifar100_train = datasets.CIFAR100(\n",
    "    root=CIFAR100_DIR,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# === Compute long-tailed distribution ===\n",
    "cls_num = len(cifar100_train.classes)  # 100\n",
    "img_max = len(cifar100_train) // cls_num  # ~500 per class\n",
    "img_num_per_cls = [\n",
    "    int(img_max * (IR ** (-i / (cls_num - 1.0)))) for i in range(cls_num)\n",
    "]\n",
    "\n",
    "# === Subsample indices ===\n",
    "targets = np.array(cifar100_train.targets)\n",
    "lt_indices = []\n",
    "for cls, num in enumerate(img_num_per_cls):\n",
    "    cls_indices = np.where(targets == cls)[0]\n",
    "    np.random.shuffle(cls_indices)\n",
    "    lt_indices.extend(cls_indices[:num])\n",
    "\n",
    "cifar100_lt_train = Subset(cifar100_train, lt_indices)\n",
    "\n",
    "# === Show stats and samples ===\n",
    "show_dataset_stats(cifar100_lt_train, name=\"CIFAR-100-LT Train\")\n",
    "print(f\"Imbalance ratio: {IR}\")\n",
    "print(\"Examples per class (first 10):\", img_num_per_cls[:10])\n",
    "\n",
    "#show_samples(cifar100_lt_train, num_samples=5) # doesn't get the sampling right\n",
    "classes = getattr(cifar100_train, \"classes\", None) # get CIFAR-100 class names, which don't live in subset\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    orig_idx = cifar100_lt_train.indices[i+1000]\n",
    "    img,label = cifar100_train[orig_idx]\n",
    "    lbl = int(label) if not torch.is_tensor(label) else int(label.item())\n",
    "    cls = classes[lbl] if (classes and 0<=lbl<len(classes)) else str(lbl)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(cls)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'cifar100-lt')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "        orig_idx = cifar100_lt_train.indices[i]\n",
    "        img,label = cifar100_train[orig_idx]\n",
    "        lbl = int(label) if not torch.is_tensor(label) else int(label.item())\n",
    "        cls = classes[lbl] if (classes and 0<=lbl<len(classes)) else str(lbl)\n",
    "        fn = os.path.join(OUTDIR, f\"{i:03d}_orig{orig_idx:05d}_{cls.replace(' ', '_')}.png\")\n",
    "        print(f'saving {fn}')\n",
    "        save_image(img, fn)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c70819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(cifar100_lt_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dafb16",
   "metadata": {},
   "source": [
    "#### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aa270",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train = datasets.CIFAR10(\n",
    "    root=CIFAR10_DIR,\n",
    "    train=True,\n",
    "    download=False,   # use local copy\n",
    "    transform=transform\n",
    ")\n",
    "cifar10_test = datasets.CIFAR10(\n",
    "    root=CIFAR10_DIR,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# === Show stats ===\n",
    "show_dataset_stats(cifar10_train, name=\"CIFAR-10 Train\")\n",
    "show_dataset_stats(cifar10_test, name=\"CIFAR-10 Test\")\n",
    "\n",
    "# === Show some sample images ===\n",
    "show_samples(cifar10_train, num_samples=5)\n",
    "\n",
    "# Save samples\n",
    "if SAVE_DATA_SAMPLES:\n",
    "    OUTDIR = os.path.join(SAVEDIR, 'cifar10')\n",
    "    os.makedirs(OUTDIR,exist_ok=True)\n",
    "    for i in range(5):\n",
    "        img, label = cifar10_train[i]\n",
    "        # save each sample next to the notebook, e.g., static_samples/CIFAR100_train/000_dog.png\n",
    "        fn = os.path.join(OUTDIR,f\"{i:03d}_{cifar10_train.classes[label]}.png\")\n",
    "        print(f'saving {fn}')\n",
    "        save_image(img,fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(cifar10_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b953a",
   "metadata": {},
   "source": [
    "### MNIST / QMNIST\n",
    "QMNIST extends MNIST with an extra 50,000 test images.  Splits test10k, test50k (aka \"extra\"), and nist can be exposed if you load QMNIST through torchvision.datasets.QMNIST with the *what* parameter. These splits are derived from the downloaded files.  \n",
    "\n",
    "In other words: \n",
    "\n",
    "- **train** - 60k (MNIST train)\n",
    "- **test10k** - 10k (the classic MNIST test set)\n",
    "- **test50k** - the ‚Äúextra‚Äù 50k (the remainder of the QMNIST test set, sometimes called \"extra\" in papers)\n",
    "- **test** - 60k total = test10k + test50k\n",
    "- **nist** - 60k additional digits sourced from NIST\n",
    "\n",
    "Source: torchvision.datasets \\\n",
    "Cite QMNIST: Yadav, C. and Bottou, L. [Cold case: The lost mnist digits.](https://proceedings.neurips.cc/paper_files/paper/2019/file/51c68dc084cb0b8467eafad1330bce66-Paper.pdf).\n",
    "In Advances in Neural Information Processing Systems\n",
    "32, 2019. \\\n",
    "Cite MNIST: LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. [Gradient-based learning applied to document recognition](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). Proceedings of the IEEE, 86(11):2278‚Äì2324, November 1998.\n",
    "\n",
    "\n",
    "#### QMNIST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmnist_train = datasets.QMNIST(root=QMNIST_DIR, what=\"train\", download=False, transform=transform)\n",
    "qmnist_test  = datasets.QMNIST(root=QMNIST_DIR, what=\"test\", download=False, transform=transform)\n",
    "qmnist_test50k  = datasets.QMNIST(root=QMNIST_DIR, what=\"test50k\", download=False, transform=transform)\n",
    "qmnist_test10k  = datasets.QMNIST(root=QMNIST_DIR, what=\"test10k\", download=False, transform=transform)\n",
    "qmnist_nist  = datasets.QMNIST(root=QMNIST_DIR, what=\"nist\", download=False, transform=transform)\n",
    "\n",
    "show_dataset_stats(qmnist_train, name=\"QMNIST Train\")\n",
    "show_dataset_stats(qmnist_test, name=\"QMNIST Test\")\n",
    "show_dataset_stats(qmnist_test50k, name=\"QMNIST Test50k\")\n",
    "show_dataset_stats(qmnist_test10k, name=\"QMNIST Test10k\")\n",
    "show_dataset_stats(qmnist_nist, name=\"QMNIST Nist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect samples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = qmnist_train[i]\n",
    "    ax.imshow(img.squeeze(0), cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "    # Save samples\n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'qmnist')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "        # save each sample next to the notebook, e.g., static_samples/CIFAR100_train/000_dog.png\n",
    "        fn = os.path.join(OUTDIR,f\"{i:03d}_{qmnist_train.classes[label].replace(' - ','_')}.png\")\n",
    "        print(f\"saving {fn}\")\n",
    "        save_image(img,fn)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f808903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(qmnist_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495b43b",
   "metadata": {},
   "source": [
    "#### MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35379353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test splits (without downloading again)\n",
    "mnist_train = datasets.MNIST(root=MNIST_DIR, train=True, download=False, transform=transform)\n",
    "mnist_test  = datasets.MNIST(root=MNIST_DIR, train=False, download=False, transform=transform)\n",
    "\n",
    "# === Show dataset statistics ===\n",
    "show_dataset_stats(mnist_train, name=\"MNIST Train\")\n",
    "show_dataset_stats(mnist_test,  name=\"MNIST Test\")\n",
    "\n",
    "# === Show sample images ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for i in range(5):\n",
    "    img, label = mnist_train[random.randint(0, len(mnist_train)-1)]\n",
    "    axes[i].imshow(img.squeeze(), cmap=\"gray\")\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis(\"off\")\n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'mnist')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "        # save each sample next to the notebook, e.g., static_samples/CIFAR100_train/000_dog.png\n",
    "        fn = os.path.join(OUTDIR,f\"{i:03d}_{mnist_train.classes[label].replace(' - ','_')}.png\")\n",
    "        print(f'saving {fn}')\n",
    "        save_image(img,fn)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(mnist_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a25a06",
   "metadata": {},
   "source": [
    "### PASCAL VOC\n",
    "\n",
    "Splits: train/validation \\\n",
    "Source: tensorflow datasets, e.g. \\\n",
    "`python -c \"import tensorflow_datasets as tfds; voc, info = tfds.load('voc/2012', data_dir='$DATASETS_DIR/vision/voc2012', with_info=True); print(info)\"` \\\n",
    "Cite: Everingham, M., Van Gool, L., Williams, C.K.I. et al. [The Pascal Visual Object Classes (VOC) Challenge](https://doi.org/10.1007/s11263-009-0275-4). Int J Comput Vis 88, 303‚Äì338 (2010). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC images come in a variety of resolutions\n",
    "# To handle them in PyTorch, we need to apply a transform\n",
    "# We permute the images to channel-last format for visualization\n",
    "# A custom collator is needed to address the different image sizes (torch.stack fails)\n",
    "\n",
    "voc_train = VOCDetection(root=VOC2012_DIR, year=\"2012\", image_set=\"train\", download=False, transform=transform)\n",
    "voc_val   = VOCDetection(root=VOC2012_DIR, year=\"2012\", image_set=\"val\", download=False, transform=transform)\n",
    "\n",
    "show_dataset_stats(voc_train, name=\"Pascal VOC 2012 Train\")\n",
    "show_dataset_stats(voc_val, name=\"Pascal VOC 2012 Validation\")\n",
    "\n",
    "# The labels in voc can get really long - like if there are 15 potted plants in an image\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def summarize_labels(labels):\n",
    "    c = Counter(labels)\n",
    "    # Pretty title like: \"dog √ó2, person, pottedplant √ó15\"\n",
    "    pretty = \", \".join([f\"{k} √ó{v}\" if v > 1 else k for k, v in c.items()])\n",
    "    # Safe slug like: \"dogx2+person+pottedplantx15\"\n",
    "    slug = \"+\".join([f\"{k}x{v}\" if v > 1 else k for k, v in c.items()])\n",
    "    # sanitize filename segment\n",
    "    slug = re.sub(r\"[^A-Za-z0-9._+-]+\", \"_\", slug)\n",
    "    return pretty, slug\n",
    "\n",
    "# Show 5 random VOC samples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = random.randint(0, len(voc_train)-1)\n",
    "    img, target = voc_train[idx]\n",
    "    ann = target[\"annotation\"]\n",
    "    objects = [obj[\"name\"] for obj in ann[\"object\"]]\n",
    "    print(f'objects: {objects}')\n",
    "    ax.imshow(np.array(img.permute(1,2,0).numpy()))\n",
    "\n",
    "    pretty, slug = summarize_labels(objects)\n",
    "    ax.set_title(pretty)\n",
    "    ax.axis(\"off\")\n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'voc2012')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "        fn = os.path.join(OUTDIR,f\"{i:03d}_{slug}.png\")\n",
    "        print(f'saving {fn}')\n",
    "        save_image(img,fn)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ead6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC images have variable resolutions \n",
    "# custom collate keeps images as a list\n",
    "def voc_collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # unzip into two tuples\n",
    "    return list(images), list(targets)\n",
    "\n",
    "# Create demo dataloader\n",
    "train_loader = DataLoader(\n",
    "    voc_train,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=voc_collate_fn  # <-- important!\n",
    ")\n",
    "# Create demo dataloader\n",
    "val_loader = DataLoader(\n",
    "    voc_val,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=voc_collate_fn  # <-- important!\n",
    ")\n",
    "\n",
    "# Inspect one batch\n",
    "images, targets = next(iter(train_loader))\n",
    "\n",
    "print(\"Number of images in batch:\", len(images))\n",
    "print(\"Image 0 size:\", images[0].size)   # original PIL size\n",
    "print(\"Image 1 size:\", images[1].size)\n",
    "\n",
    "print(\"Targets type:\", type(targets), \"Length:\", len(targets))\n",
    "for i, t in enumerate(targets):\n",
    "    ann = t[\"annotation\"]\n",
    "    objects = [obj[\"name\"] for obj in ann[\"object\"]]\n",
    "    print(f\"Sample {i} objects: {objects}\")\n",
    "\n",
    "print(\"Number of batches in train loader:\", len(train_loader))\n",
    "print(\"Number of batches in val loader:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c7534",
   "metadata": {},
   "source": [
    "### CINIC-10\n",
    "Splits: train/validation/test \\\n",
    "Source: https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz \\\n",
    "Cite: Darlow, Luke N; Crowley, Elliot J; Antoniou, Antreas; Storkey, Amos.(2018). [CINIC-10 Is Not ImageNet or CIFAR-10](https://datashare.ed.ac.uk/handle/10283/3192), [dataset]. University of Edinburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5616f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CINIC-10 splits\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "cinic_train = datasets.ImageFolder(root=os.path.join(CINIC10_DIR, \"train\"), transform=transform)\n",
    "cinic_val   = datasets.ImageFolder(root=os.path.join(CINIC10_DIR, \"valid\"), transform=transform)\n",
    "cinic_test  = datasets.ImageFolder(root=os.path.join(CINIC10_DIR, \"test\"),  transform=transform)\n",
    "\n",
    "# === Show dataset statistics ===\n",
    "show_dataset_stats(cinic_train, name=\"CINIC-10 Train\")\n",
    "show_dataset_stats(cinic_val,   name=\"CINIC-10 Validation\")\n",
    "show_dataset_stats(cinic_test,  name=\"CINIC-10 Test\")\n",
    "\n",
    "# === Show sample images ===\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for i in range(5):\n",
    "    img, label = cinic_train[random.randint(0, len(cinic_train)-1)]\n",
    "    axes[i].imshow(img.permute(1, 2, 0))\n",
    "    axes[i].set_title(f\"Label: {cinic_train.classes[label]}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'CINIC-10')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "        # save each sample next to the notebook, e.g., static_samples/CIFAR100_train/000_dog.png\n",
    "        fn = os.path.join(OUTDIR,f\"{i:03d}_{label}.png\")\n",
    "        print(f'saving {fn}')\n",
    "        save_image(img,fn)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(cinic_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3b57b",
   "metadata": {},
   "source": [
    "### SVHN (Street View House Numbers)\n",
    "\n",
    "Splits: train/test \\\n",
    "Source: torchvision.datasets \\\n",
    "Cite: Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. [Reading digits in natural images with\n",
    "unsupervised feature learning](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37648.pdf). NIPS workshop on deep learning and unsupervised feature learning. Vol. 2011. No. 5. 2011.\n",
    "\n",
    "**Note** \\\n",
    "Unlike MNIST or QMNIST, the **SVHN dataset** does not create `raw/` and `processed/` subdirectories.  \n",
    "It ships as ready-to-use MATLAB `.mat` files:\n",
    "\n",
    "- `train_32x32.mat`\n",
    "- `test_32x32.mat`\n",
    "- (optionally) `extra_32x32.mat`\n",
    "\n",
    "The `torchvision.datasets.SVHN` class knows how to read these `.mat` files directly, so as long as they are present in the dataset root, you can load the dataset with:\n",
    "\n",
    "```python\n",
    "svhn_train = datasets.SVHN(root=SVHN_DIR, split=\"train\", download=False, transform=transform)\n",
    "```\n",
    "Finally, note that the SVHN set provided by torchvision.datasets is the \"cropped\" version of the [original Stanford dataset](http://ufldl.stanford.edu/housenumbers/).  The resolution is lower, and the class labels are a **single** digit - so if the visible street number is 714, the label will likely be the single digit 1.  The source files of the full dataset, with multi-digit labels and bounding boxes, are available in our shared datasets directory, but the loading is more complex, and as most studies seem to reference the cropped set, a dataloader has not yet been provided in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157db733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test splits\n",
    "svhn_train = datasets.SVHN(root=SVHN_DIR, split=\"train\", download=False, transform=transform)\n",
    "svhn_test  = datasets.SVHN(root=SVHN_DIR, split=\"test\", download=False, transform=transform)\n",
    "\n",
    "# Show dataset statistics\n",
    "show_dataset_stats(svhn_train, name=\"SVHN Train\")\n",
    "show_dataset_stats(svhn_test,  name=\"SVHN Test\")\n",
    "\n",
    "# Show sample images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for i in range(5):\n",
    "    img, label = svhn_train[random.randint(0, len(svhn_train)-1)]\n",
    "    axes[i].imshow(img.permute(1, 2, 0))\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis(\"off\")\n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'svhn')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "        fn = os.path.join(OUTDIR,f\"{i:03d}_{label}.png\")\n",
    "        print(f'saving {fn}')\n",
    "        save_image(img,fn)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(svhn_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31590694",
   "metadata": {},
   "source": [
    "### Clothing-1M\n",
    "This is a really big dataset, so it hasn't been expanded just yet.  Has been downloaded and can be expanded if necessary.\n",
    "\n",
    "Cite: Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X. [Learning from massive noisy labeled data for image classification](https://openaccess.thecvf.com/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf). In CVPR, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a60fd",
   "metadata": {},
   "source": [
    "### Tiny ImageNet\n",
    "Splits: train/val/test \\\n",
    "Source: torchvision.datasets \\\n",
    "Cite Tiny ImageNet challenge: Le, Yann, and Xuan Yang. \"[Tiny imagenet visual recognition challenge](https://cs231n.stanford.edu/reports/2015/pdfs/yle_project.pdf).\" CS 231N 7.7 (2015): 3 \\\n",
    "Cite Tiny ImageNet original:  Deng, Jia, et al. \"[ImageNet: A Large-Scale Hierarchical Image Database](https://ieeexplore.ieee.org/document/5206848).\" 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248‚Äì255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(TINY_IMAGENET_DIR, \"train\")\n",
    "val_dir   = os.path.join(TINY_IMAGENET_DIR, \"val\")\n",
    "test_dir   = os.path.join(TINY_IMAGENET_DIR, \"test\")\n",
    "\n",
    "tiny_train = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "tiny_val   = datasets.ImageFolder(root=val_dir,   transform=transform)\n",
    "tiny_test   = datasets.ImageFolder(root=test_dir,   transform=transform)\n",
    "\n",
    "print(\"üìä Tiny ImageNet\")\n",
    "print(\"Train samples:\", len(tiny_train))\n",
    "print(\"Validation samples:\", len(tiny_val))\n",
    "print(\"Test samples:\", len(tiny_test))\n",
    "print(\"Number of classes:\", len(tiny_train.classes))\n",
    "\n",
    "# Helpers to get human-readable classes if needed for saving\n",
    "from pathlib import Path\n",
    "root = Path(TINY_IMAGENET_DIR)\n",
    "wnid_to_words = {}\n",
    "with open (root / \"words.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        wnid,words = line.strip().split(\"\\t\", 1)\n",
    "        wnid_to_words[wnid] = words\n",
    "        \n",
    "idx_to_wnid = {i: wnid for i, wnid in enumerate(tiny_train.classes)}\n",
    "idx_to_name = {i: wnid_to_words.get(wnid, wnid) for i, wnid in idx_to_wnid.items()}\n",
    "        \n",
    "\n",
    "# Labels for tiny imagenet are directory names, like \"n02814860,\" that hold same-class images.  \n",
    "# To get a human-readable image, you need to read the dataset files wnids.txt and words.txt.  \n",
    "# Show a few random samples from training\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "for i in range(5):\n",
    "#    img, label = random.choice(tiny_train.samples)\n",
    "    img_t, label = tiny_train[random.randrange(len(tiny_train))]\n",
    "#    image = plt.imread(img)\n",
    "#    axes[i].imshow(image)\n",
    "#    axes[i].set_title(tiny_train.classes[label])\n",
    "    axes[i].imshow(img_t.permute(1,2,0))\n",
    "    axes[i].set_title(idx_to_name[label])\n",
    "    axes[i].axis(\"off\")\n",
    "    \n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'tiny-imagenet-200')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "        fn = os.path.join(OUTDIR,f\"{i:03d}_{idx_to_name[label]}.png\")\n",
    "        print(f'saving {fn}')\n",
    "        save_image(img_t,fn)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(tiny_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99184b21",
   "metadata": {},
   "source": [
    "### ImageNet 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59207b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce consistent image size\n",
    "imagenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),             # resize shorter side to 256\n",
    "    transforms.CenterCrop(224),         # crop to 224x224 (standard ImageNet size)\n",
    "    transforms.ToTensor(),              # convert to tensor [0,1]\n",
    "    transforms.Normalize(               # normalize with ImageNet stats\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "imagenet_train = datasets.ImageFolder(root=os.path.join(IMAGENET_DIR, \"train\"), transform=imagenet_transform)\n",
    "imagenet_val   = datasets.ImageFolder(root=os.path.join(IMAGENET_DIR, \"val\"),   transform=imagenet_transform)\n",
    "\n",
    "# Helpers to get human-readable classes\n",
    "\n",
    "from pathlib import Path\n",
    "import scipy.io as sio\n",
    "import re\n",
    "\n",
    "# build WNID -> name dict from devkit\n",
    "dev = Path(IMAGENET_DIR) / \"ILSVRC2012_devkit_t12\" / \"data\" / \"meta.mat\"\n",
    "synsets = sio.loadmat(dev, squeeze_me=True)[\"synsets\"]\n",
    "wnid_to_name = { str(s[\"WNID\"]).strip(): str(s[\"words\"]).strip() for s in synsets }\n",
    "\n",
    "idx_to_syn  = {i: syn for i, syn in enumerate(imagenet_train.classes)}\n",
    "idx_to_name = {i: wnid_to_name.get(idx_to_syn[i], idx_to_syn[i]) for i in idx_to_syn}\n",
    "\n",
    "def unnorm_imgnet(x):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)        \n",
    "    return(x*std + mean).clamp(0,1)\n",
    "\n",
    "\n",
    "# Show dataset stats\n",
    "print(\"üìä ImageNet-2012\")\n",
    "print(\"Train samples:\", len(imagenet_train))\n",
    "print(\"Validation samples:\", len(imagenet_val))\n",
    "print(\"Number of classes:\", len(imagenet_train.classes))\n",
    "\n",
    "# Show a few random validation samples\n",
    "#fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "#for i in range(5):\n",
    "#    img_path, label_idx = random.choice(imagenet_val.samples)\n",
    "#    image = plt.imread(img_path)\n",
    "#    axes[i].imshow(image)\n",
    "#    title = idx_to_name[label_idx]\n",
    "#    axes[i].set_title(title)\n",
    "#    axes[i].axis(\"off\")\n",
    "#    if SAVE_DATA_SAMPLES:\n",
    "#        OUTDIR = os.path.join(SAVEDIR, 'imagenet-2012')\n",
    "#        os.makedirs(OUTDIR,exist_ok=True)\n",
    "#        fn = os.path.join(OUTDIR,f\"{i:03d}_{safe(title)}.png\")\n",
    "#        print(f'saving {fn}')\n",
    "#        save_image(unnorm_imgnet(img_path),fn)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "for i in range(5):\n",
    "    x_t, label_idx = imagenet_val[random.randrange(len(imagenet_val))]  # x_t: CHW normalized tensor\n",
    "    x_vis = unnorm_imgnet(x_t)\n",
    "    title = idx_to_name[label_idx]\n",
    "\n",
    "    axes[i].imshow(x_vis.permute(1, 2, 0))\n",
    "    axes[i].set_title(title); axes[i].axis(\"off\")\n",
    "\n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'imagenet-2012')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "\n",
    "        fn = os.path.join(OUTDIR, f\"{i:03d}_{safe(title)}.png\")\n",
    "        save_image(x_vis, fn)  # tensor in [0,1]\n",
    "        print(\"saved\", fn)\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9cf9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader \n",
    "train_loader = DataLoader(imagenet_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # e.g. [4, 3, 32, 32]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b65c0",
   "metadata": {},
   "source": [
    "### Wikipedia\n",
    "Hong references the Wikpedia dataset.  However, the citations are Rasiwasia et al., 2010; Hu et al.,\n",
    "2021, which refer to a ~3000 item text-image dataset described here: http://www.svcl.ucsd.edu/projects/crossmodal/\n",
    "and not the \"Wikipedia\" dataset available from HuggingFace.  \n",
    "\n",
    "This shows the cited Rasiwaisa wikipedia version.  Note that entities are not always clean, hence safe equivalents in read_text, e.g. nbsp -> mdash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# === Categories ===\n",
    "cat_file = os.path.join(WIKI_DIR, \"categories.list\")\n",
    "with open(cat_file, \"r\") as f:\n",
    "    categories = [line.strip() for line in f]\n",
    "print(\"Categories:\", categories)\n",
    "\n",
    "# === Split files ===\n",
    "def load_split_list(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        return [line.strip().split() for line in f]\n",
    "\n",
    "train_list = load_split_list(os.path.join(WIKI_DIR, \"trainset_txt_img_cat.list\"))\n",
    "test_list  = load_split_list(os.path.join(WIKI_DIR, \"testset_txt_img_cat.list\"))\n",
    "\n",
    "print(\"Train samples:\", len(train_list))\n",
    "print(\"Test samples:\", len(test_list))\n",
    "\n",
    "# === XML-aware text reader ===\n",
    "def read_text(txt_file):\n",
    "    base_path = os.path.join(WIKI_DIR, \"texts\", txt_file)\n",
    "    candidates = [base_path, base_path + \".xml\"]\n",
    "\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    xml_str = f.read()\n",
    "                # Replace common problematic entities\n",
    "                xml_str = xml_str.replace(\"&nbsp;\", \" \").replace(\"&mdash;\", \"-\")\n",
    "                xml_str = xml_str.replace(\"&ndash;\", \"-\").replace(\"&amp;\", \"&\")\n",
    "\n",
    "                root = ET.fromstring(xml_str)\n",
    "                text_elements = root.findall(\".//text\")\n",
    "                if text_elements:\n",
    "                    return \"\\n\\n\".join(el.text for el in text_elements if el.text)\n",
    "                else:\n",
    "                    return \"\\n\\n\".join(root.itertext())\n",
    "            except Exception as e:\n",
    "                return f\"[Error parsing {os.path.basename(path)}: {e}]\"\n",
    "\n",
    "    return f\"[Text not found: {txt_file}]\"\n",
    "\n",
    "# === Image finder ===\n",
    "def find_image(img_fname):\n",
    "    for cat in categories:\n",
    "        candidate = os.path.join(WIKI_DIR, \"images\", cat, f\"{img_fname}.jpg\")\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate, cat\n",
    "    return None, None\n",
    "\n",
    "# === Show 5 random samples ===\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "shown = 0\n",
    "for sample in random.sample(train_list, 15):  # pull extras to avoid missing\n",
    "    if shown >= 5:\n",
    "        break\n",
    "    txt_fname, img_fname, class_id = sample\n",
    "    img_path, found_cat = find_image(img_fname)\n",
    "    if img_path is None:\n",
    "        continue\n",
    "    text = read_text(txt_fname)\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    axes[shown].imshow(image)\n",
    "    axes[shown].set_title(found_cat)\n",
    "    axes[shown].axis(\"off\")\n",
    "    print(f\"Text snippet {shown+1} ({found_cat}): {text[:120]}...\")\n",
    "    shown += 1\n",
    "    if SAVE_DATA_SAMPLES:\n",
    "        OUTDIR = os.path.join(SAVEDIR, 'wikipedia-vision')\n",
    "        os.makedirs(OUTDIR,exist_ok=True)\n",
    "\n",
    "        fn = os.path.join(OUTDIR, f\"{shown:03d}_{safe(found_cat or 'unknown')}.png\")\n",
    "        image.save(fn)\n",
    "        print(\"saved\", fn)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568039d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did not come DataLoader-ready so needs a custom dataset wrapper\n",
    "\n",
    "# Define a transform (resize all images, convert to tensor)\n",
    "wiki_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # force all images to same size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "    \n",
    "class WikipediaDataset(Dataset):\n",
    "    def __init__(self, samples, categories, transform=None):\n",
    "        self.samples = samples\n",
    "        self.categories = categories\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt_fname, img_fname, class_id = self.samples[idx]\n",
    "\n",
    "        # image lookup\n",
    "        img_path, found_cat = find_image(img_fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # text lookup\n",
    "        text = read_text(txt_fname)\n",
    "\n",
    "        return image, text, int(class_id), found_cat\n",
    "\n",
    "\n",
    "# --- Build DataLoader ---\n",
    "wiki_train = WikipediaDataset(train_list, categories, transform=wiki_transform)\n",
    "train_loader = DataLoader(wiki_train, batch_size=2, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "images, texts, labels, cats = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch images shape:\", images.shape)       # e.g. [2, 3, 224, 224]\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Batch categories:\", cats)\n",
    "print(\"Text snippet 1:\", texts[0][:100], \"...\")\n",
    "print(\"Number of batches in train loader:\", len(train_loader))\n",
    "\n",
    "# ‚ö†Ô∏è Note: texts are returned as a list of strings (not collated into tensors).\n",
    "# This is expected ‚Äî it keeps variable-length text visible for inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d7ccc",
   "metadata": {},
   "source": [
    "## NLP Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d09e08",
   "metadata": {},
   "source": [
    "### CoLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a998836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will load from our local copy unless it is not found, in which case it loads from HuggingFace\n",
    "cola = load_dataset(\"glue\", \"cola\", cache_dir=COLA_DIR)\n",
    "show_split_sizes_hf(cola, \"CoLA\", label_field=\"label\")\n",
    "\n",
    "# Print label mapping\n",
    "print(\"Label meanings: 0 = unacceptable, 1 = acceptable\")\n",
    "\n",
    "# Show a few rows explicitly (pandas)\n",
    "#df = pd.DataFrame(cola[\"train\"][:5])\n",
    "df = pd.DataFrame(cola[\"train\"].shuffle(seed=42).select(range(5)))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader\n",
    "torch_cola_train = hf_to_torch(cola[\"train\"])\n",
    "train_loader = DataLoader(torch_cola_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "texts, labels = next(iter(train_loader))\n",
    "print(\"Batch texts:\", texts)   # list of strings\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89c546",
   "metadata": {},
   "source": [
    "### SST2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will load from our local copy unless it is not found, in which case it loads from HuggingFace\n",
    "sst2 = load_dataset(\"glue\", \"sst2\", cache_dir=SST2_DIR)\n",
    "\n",
    "# Show split sizes\n",
    "show_split_sizes_hf(sst2, \"SST-2\", label_field=\"label\")\n",
    "\n",
    "# Print label mapping\n",
    "print(\"Label meanings: 0 = negative, 1 = positive\")\n",
    "\n",
    "# Display 5 random examples from train\n",
    "df = pd.DataFrame(sst2[\"train\"].shuffle(seed=42).select(range(5)))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataloader\n",
    "torch_sst2_train = hf_to_torch(sst2[\"train\"])\n",
    "train_loader = DataLoader(torch_sst2_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "texts, labels = next(iter(train_loader))\n",
    "print(\"Batch texts:\", texts)   # list of strings\n",
    "print(\"Batch labels:\", labels.tolist())\n",
    "print(\"Number of batches in train loader:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a6dca",
   "metadata": {},
   "source": [
    "### E2E NLG\n",
    "The task is to generate natural-sounding restaurant descriptions directly from structured meaning representations (MRs)\n",
    "\n",
    "Note that this dataset contains two testsets.  testset.csv has a single human reference per MR.  testset_w_refs has MULTIPLE human references for the same MR.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file downloaded to shared storage and expanded to CSVs\n",
    "# Load splits separately (so schema mismatch is allowed - inconsistent column names)\n",
    "train = load_dataset(\"csv\", data_files=os.path.join(E2ENLG_DIR, \"trainset.csv\"))[\"train\"]\n",
    "val   = load_dataset(\"csv\", data_files=os.path.join(E2ENLG_DIR, \"devset.csv\"))[\"train\"] # quirk of e2e_nlg that \"train\" just indicates full set\n",
    "test  = load_dataset(\"csv\", data_files=os.path.join(E2ENLG_DIR, \"testset.csv\"))[\"train\"]\n",
    "\n",
    "# Normalize column names (lowercase)\n",
    "if \"MR\" in test.column_names:\n",
    "    test = test.rename_column(\"MR\", \"mr\")\n",
    "if \"REF\" in test.column_names:\n",
    "    test = test.rename_column(\"REF\", \"ref\")\n",
    "\n",
    "# Wrap into a dict for convenience\n",
    "e2e = {\"train\": train, \"validation\": val, \"test\": test}\n",
    "\n",
    "# === Show stats ===\n",
    "print(\"üìä E2E NLG\")\n",
    "for split, ds in e2e.items():\n",
    "    print(f\"{split}: {len(ds)} samples\")\n",
    "\n",
    "# Print column descriptions\n",
    "print(\"\\nField meanings:\")\n",
    "print(\" - mr  = meaning representation (structured input)\")\n",
    "print(\" - ref = reference text (human-written realization)\")\n",
    "\n",
    "# Show 5 random rows from train\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(train.shuffle(seed=42).select(range(5)))\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e4ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified helper for E2E\n",
    "def hf_to_torch_e2e(dataset):\n",
    "    return [(ex[\"mr\"], ex[\"ref\"]) for ex in dataset]\n",
    "\n",
    "# Create demo dataloader\n",
    "torch_e2e_train = hf_to_torch_e2e(e2e[\"train\"])\n",
    "train_loader = DataLoader(torch_e2e_train, batch_size=4, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "mrs, refs = next(iter(train_loader))\n",
    "print(\"Batch MRs:\", mrs)        # list of meaning representations\n",
    "print(\"Batch Refs:\", refs)      # list of reference utterances\n",
    "print(\"Number of batches in train loader:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220e359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (haag_batchselection)",
   "language": "python",
   "name": "my_venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Training with Loss-Based Batch Selection\n",
    "\n",
    "This notebook demonstrates training BERT on IMDB with two strategies:\n",
    "1. **Normal**: Standard training on all examples\n",
    "2. **Loss-based**: Filter examples based on previous loss\n",
    "\n",
    "**Note:** For quick testing in Colab, we'll use a subset of data. Remove the subset limits for full training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets scipy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Training configuration\n",
    "MAX_LENGTH = 128  # Reduced for faster training in Colab\n",
    "BATCH_SIZE = 8    # Reduced for Colab GPU memory\n",
    "EPOCHS = 3\n",
    "N_RUNS = 1        # Set to 1 for quick demo (use 3+ for robust results)\n",
    "LEARNING_RATE = 2e-5\n",
    "LOSS_THRESHOLD = 0.5\n",
    "\n",
    "# For quick testing, use subset of data\n",
    "USE_SUBSET = True\n",
    "TRAIN_SUBSET_SIZE = 1000  # Use 1000 samples for quick demo\n",
    "TEST_SUBSET_SIZE = 500\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Max Length: {MAX_LENGTH}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Runs: {N_RUNS}\")\n",
    "print(f\"  Using Subset: {USE_SUBSET}\")\n",
    "if USE_SUBSET:\n",
    "    print(f\"  Train Subset: {TRAIN_SUBSET_SIZE}\")\n",
    "    print(f\"  Test Subset: {TEST_SUBSET_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"Wrapper for HuggingFace IMDB dataset with tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, split='train', max_length=128, subset_size=None):\n",
    "        print(f\"Loading {split} dataset...\")\n",
    "        self.dataset = load_dataset('stanfordnlp/imdb', split=split)\n",
    "        \n",
    "        # Use subset if specified\n",
    "        if subset_size is not None:\n",
    "            indices = list(range(min(subset_size, len(self.dataset))))\n",
    "            self.dataset = self.dataset.select(indices)\n",
    "            print(f\"Using subset of {len(self.dataset)} samples\")\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item['text']\n",
    "        label = item['label']\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_batch_sampler(dataset, batch_size, **kwargs):\n",
    "    \"\"\"Standard random batch sampling.\"\"\"\n",
    "    n = len(dataset)\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    for start in range(0, n, batch_size):\n",
    "        yield indices[start:start + batch_size].tolist()\n",
    "\n",
    "\n",
    "def loss_based_batch_sampler(dataset, batch_size, loss_history=None, threshold=LOSS_THRESHOLD, **kwargs):\n",
    "    \"\"\"\n",
    "    Sample batches from examples with loss above threshold.\n",
    "    First epoch: uses all examples.\n",
    "    Subsequent epochs: only examples with loss > threshold.\n",
    "\n",
    "    Always ensures at least batch_size examples are selected to prevent empty epochs.\n",
    "    \"\"\"\n",
    "    if loss_history is None or loss_history.sum() == 0:\n",
    "        # First epoch: use all examples\n",
    "        indices = np.arange(len(dataset))\n",
    "    else:\n",
    "        # Filter to examples with loss above threshold\n",
    "        indices = np.where(loss_history > threshold)[0]\n",
    "\n",
    "        # IMPORTANT: Always ensure minimum number of samples to prevent empty epochs\n",
    "        min_samples = batch_size\n",
    "        if len(indices) < min_samples:\n",
    "            # Not enough samples - pick top-k by loss instead\n",
    "            top_k = min(min_samples, len(dataset))\n",
    "            indices = np.argsort(loss_history)[-top_k:]  # Top-k highest loss samples\n",
    "            print(f\"  Filtering: Only {len(np.where(loss_history > threshold)[0])} examples above threshold, using top-{top_k} by loss instead\")\n",
    "        else:\n",
    "            print(f\"  Filtering: {len(indices)}/{len(dataset)} examples above threshold\")\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Yield batches, including any remaining samples in a final smaller batch\n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[start:start + batch_size]\n",
    "        yield batch_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, test_dataset, epochs, batch_size,\n",
    "                batch_sampler_fn, strategy_name, seed=None):\n",
    "    \"\"\"Train BERT model with specified batch sampling strategy.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Calculate total training steps for scheduler\n",
    "    n_samples = len(train_dataset)\n",
    "    steps_per_epoch = n_samples // batch_size\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Track per-sample loss\n",
    "    per_sample_loss = np.zeros(len(train_dataset))\n",
    "\n",
    "    # Metrics storage\n",
    "    train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "    samples_per_epoch = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct, n_samples_epoch, running_loss = 0, 0, 0\n",
    "\n",
    "        # Get batch iterator\n",
    "        if 'loss_history' in batch_sampler_fn.__code__.co_varnames:\n",
    "            batch_iter = batch_sampler_fn(train_dataset, batch_size, loss_history=per_sample_loss)\n",
    "        else:\n",
    "            batch_iter = batch_sampler_fn(train_dataset, batch_size)\n",
    "\n",
    "        # Training loop\n",
    "        for batch_indices in batch_iter:\n",
    "            # Prepare batch\n",
    "            batch = [train_dataset[i] for i in batch_indices]\n",
    "            input_ids = torch.stack([item['input_ids'] for item in batch]).to(DEVICE)\n",
    "            attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(DEVICE)\n",
    "            labels = torch.stack([item['label'] for item in batch]).to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            # Compute loss per sample\n",
    "            logits = outputs.logits\n",
    "            loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "            losses_per_sample = loss_fn(logits, labels)\n",
    "            loss = losses_per_sample.mean()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update metrics\n",
    "            running_loss += loss.item() * len(batch_indices)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            n_samples_epoch += len(batch_indices)\n",
    "\n",
    "            # Update per-sample loss history\n",
    "            if 'loss_history' in batch_sampler_fn.__code__.co_varnames:\n",
    "                for k, idx in enumerate(batch_indices):\n",
    "                    per_sample_loss[idx] = losses_per_sample[k].item()\n",
    "\n",
    "        # Epoch metrics\n",
    "        train_acc = correct / n_samples_epoch if n_samples_epoch > 0 else 0\n",
    "        train_loss = running_loss / n_samples_epoch if n_samples_epoch > 0 else 0\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        samples_per_epoch.append(n_samples_epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        test_acc, test_loss = evaluate(model, test_dataset, batch_size)\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"[{strategy_name}] Epoch {epoch+1}/{epochs}: \"\n",
    "              f\"train_acc={train_acc:.4f}, test_acc={test_acc:.4f}, \"\n",
    "              f\"train_loss={train_loss:.4f}, samples={n_samples_epoch}\")\n",
    "\n",
    "    return {\n",
    "        'train_acc': train_accs,\n",
    "        'test_acc': test_accs,\n",
    "        'train_loss': train_losses,\n",
    "        'test_loss': test_losses,\n",
    "        'samples_per_epoch': samples_per_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, batch_size):\n",
    "    \"\"\"Evaluate model on dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct, total, total_loss = 0, 0, 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[start:start + batch_size]\n",
    "            batch = [dataset[i] for i in batch_indices]\n",
    "\n",
    "            input_ids = torch.stack([item['input_ids'] for item in batch]).to(DEVICE)\n",
    "            attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(DEVICE)\n",
    "            labels = torch.stack([item['label'] for item in batch]).to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "\n",
    "            total_loss += loss.item() * len(batch_indices)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(batch_indices)\n",
    "\n",
    "    return correct / total, total_loss / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = IMDBDataset(\n",
    "    split='train',\n",
    "    max_length=MAX_LENGTH,\n",
    "    subset_size=TRAIN_SUBSET_SIZE if USE_SUBSET else None\n",
    ")\n",
    "\n",
    "test_dataset = IMDBDataset(\n",
    "    split='test',\n",
    "    max_length=MAX_LENGTH,\n",
    "    subset_size=TEST_SUBSET_SIZE if USE_SUBSET else None\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Experiments\n",
    "\n",
    "### Strategy 1: Normal (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STRATEGY 1: NORMAL (BASELINE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create fresh model\n",
    "model_normal = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "results_normal = train_model(\n",
    "    model_normal, train_dataset, test_dataset,\n",
    "    EPOCHS, BATCH_SIZE,\n",
    "    normal_batch_sampler, \"Normal\",\n",
    "    seed=42\n",
    ")\n",
    "time_normal = time.time() - start_time\n",
    "\n",
    "print(f\"\\nNormal strategy completed in {time_normal:.2f}s\")\n",
    "print(f\"Final test accuracy: {results_normal['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Loss-Based Batch Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY 2: LOSS-BASED BATCH SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create fresh model\n",
    "model_loss_based = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "results_loss_based = train_model(\n",
    "    model_loss_based, train_dataset, test_dataset,\n",
    "    EPOCHS, BATCH_SIZE,\n",
    "    loss_based_batch_sampler, \"Loss-Based\",\n",
    "    seed=42\n",
    ")\n",
    "time_loss_based = time.time() - start_time\n",
    "\n",
    "print(f\"\\nLoss-based strategy completed in {time_loss_based:.2f}s\")\n",
    "print(f\"Final test accuracy: {results_loss_based['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "epochs_range = np.arange(1, EPOCHS + 1)\n",
    "\n",
    "# Test Accuracy\n",
    "axes[0, 0].plot(epochs_range, results_normal['test_acc'], 'o-', label='Normal', linewidth=2)\n",
    "axes[0, 0].plot(epochs_range, results_loss_based['test_acc'], 's-', label='Loss-Based', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Test Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Train Loss\n",
    "axes[0, 1].plot(epochs_range, results_normal['train_loss'], 'o-', label='Normal', linewidth=2)\n",
    "axes[0, 1].plot(epochs_range, results_loss_based['train_loss'], 's-', label='Loss-Based', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Train Loss')\n",
    "axes[0, 1].set_title('Train Loss Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Train Accuracy\n",
    "axes[1, 0].plot(epochs_range, results_normal['train_acc'], 'o-', label='Normal', linewidth=2)\n",
    "axes[1, 0].plot(epochs_range, results_loss_based['train_acc'], 's-', label='Loss-Based', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Train Accuracy')\n",
    "axes[1, 0].set_title('Train Accuracy Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Samples per Epoch\n",
    "axes[1, 1].plot(epochs_range, results_normal['samples_per_epoch'], 'o-', label='Normal', linewidth=2)\n",
    "axes[1, 1].plot(epochs_range, results_loss_based['samples_per_epoch'], 's-', label='Loss-Based', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Number of Samples')\n",
    "axes[1, 1].set_title('Samples per Epoch')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nNormal Strategy:\")\n",
    "print(f\"  Final Test Accuracy: {results_normal['test_acc'][-1]:.4f}\")\n",
    "print(f\"  Training Time: {time_normal:.2f}s\")\n",
    "print(f\"  Samples per epoch: {results_normal['samples_per_epoch']}\")\n",
    "\n",
    "print(f\"\\nLoss-Based Strategy:\")\n",
    "print(f\"  Final Test Accuracy: {results_loss_based['test_acc'][-1]:.4f}\")\n",
    "print(f\"  Training Time: {time_loss_based:.2f}s\")\n",
    "print(f\"  Samples per epoch: {results_loss_based['samples_per_epoch']}\")\n",
    "\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"  - Loss-based filtering reduces samples in later epochs\")\n",
    "print(f\"  - Potential speedup: {(time_normal/time_loss_based):.2f}x\")\n",
    "print(f\"  - Accuracy difference: {(results_loss_based['test_acc'][-1] - results_normal['test_acc'][-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10b. Training Time & Compute Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate compute metrics\n",
    "total_samples_normal = sum(results_normal['samples_per_epoch'])\n",
    "total_samples_loss_based = sum(results_loss_based['samples_per_epoch'])\n",
    "\n",
    "# Compute savings\n",
    "time_saved = time_normal - time_loss_based\n",
    "time_saved_percent = (time_saved / time_normal) * 100\n",
    "compute_saved = total_samples_normal - total_samples_loss_based\n",
    "compute_saved_percent = (compute_saved / total_samples_normal) * 100\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Total Training Time Bar Chart\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "strategies = ['Normal', 'Loss-Based']\n",
    "times = [time_normal, time_loss_based]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "bars = ax1.bar(strategies, times, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Total Training Time', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}s',\n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 2. Time Savings\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.text(0.5, 0.6, f'{time_saved:.1f}s', \n",
    "         ha='center', va='center', fontsize=48, fontweight='bold', color='#27ae60')\n",
    "ax2.text(0.5, 0.35, f'({time_saved_percent:.1f}% faster)', \n",
    "         ha='center', va='center', fontsize=20, color='#27ae60')\n",
    "ax2.text(0.5, 0.15, 'Time Saved', \n",
    "         ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Time Savings', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 3. Speedup Factor\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "speedup = time_normal / time_loss_based if time_loss_based > 0 else 1.0\n",
    "ax3.text(0.5, 0.6, f'{speedup:.2f}x', \n",
    "         ha='center', va='center', fontsize=48, fontweight='bold', color='#8e44ad')\n",
    "ax3.text(0.5, 0.35, 'Speedup Factor', \n",
    "         ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "ax3.set_xlim(0, 1)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Training Speedup', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 4. Total Compute Usage (Total Samples Processed)\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "total_samples = [total_samples_normal, total_samples_loss_based]\n",
    "bars = ax4.bar(strategies, total_samples, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax4.set_ylabel('Total Samples Processed', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Total Compute Usage', fontsize=14, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}',\n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 5. Compute Savings\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.text(0.5, 0.6, f'{int(compute_saved):,}', \n",
    "         ha='center', va='center', fontsize=40, fontweight='bold', color='#27ae60')\n",
    "ax5.text(0.5, 0.35, f'({compute_saved_percent:.1f}% reduction)', \n",
    "         ha='center', va='center', fontsize=20, color='#27ae60')\n",
    "ax5.text(0.5, 0.15, 'Samples Saved', \n",
    "         ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "ax5.set_xlim(0, 1)\n",
    "ax5.set_ylim(0, 1)\n",
    "ax5.axis('off')\n",
    "ax5.set_title('Compute Savings', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 6. Cumulative Samples Over Epochs\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "cumsum_normal = np.cumsum(results_normal['samples_per_epoch'])\n",
    "cumsum_loss_based = np.cumsum(results_loss_based['samples_per_epoch'])\n",
    "epochs_range = np.arange(1, EPOCHS + 1)\n",
    "\n",
    "ax6.plot(epochs_range, cumsum_normal, 'o-', label='Normal', \n",
    "         linewidth=3, markersize=8, color='#3498db')\n",
    "ax6.plot(epochs_range, cumsum_loss_based, 's-', label='Loss-Based', \n",
    "         linewidth=3, markersize=8, color='#e74c3c')\n",
    "ax6.fill_between(epochs_range, cumsum_normal, cumsum_loss_based, \n",
    "                  alpha=0.3, color='#27ae60', label='Compute Saved')\n",
    "ax6.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax6.set_ylabel('Cumulative Samples', fontsize=12, fontweight='bold')\n",
    "ax6.set_title('Cumulative Compute Usage', fontsize=14, fontweight='bold')\n",
    "ax6.legend(fontsize=10)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Samples per Epoch Breakdown\n",
    "ax7 = fig.add_subplot(gs[2, :2])\n",
    "x = np.arange(EPOCHS)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax7.bar(x - width/2, results_normal['samples_per_epoch'], width, \n",
    "                label='Normal', color='#3498db', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax7.bar(x + width/2, results_loss_based['samples_per_epoch'], width, \n",
    "                label='Loss-Based', color='#e74c3c', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax7.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax7.set_ylabel('Samples Processed', fontsize=12, fontweight='bold')\n",
    "ax7.set_title('Samples Processed per Epoch', fontsize=14, fontweight='bold')\n",
    "ax7.set_xticks(x)\n",
    "ax7.set_xticklabels([f'Epoch {i+1}' for i in range(EPOCHS)])\n",
    "ax7.legend(fontsize=11)\n",
    "ax7.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax7.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{int(height)}',\n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 8. Efficiency Metrics Summary Table\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "ax8.axis('tight')\n",
    "ax8.axis('off')\n",
    "\n",
    "# Create summary table data\n",
    "table_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Time Saved', f'{time_saved:.1f}s'],\n",
    "    ['Speedup', f'{speedup:.2f}x'],\n",
    "    ['Samples Saved', f'{int(compute_saved):,}'],\n",
    "    ['Compute Reduction', f'{compute_saved_percent:.1f}%'],\n",
    "    ['Final Accuracy Diff', f'{(results_loss_based[\"test_acc\"][-1] - results_normal[\"test_acc\"][-1]):.4f}']\n",
    "]\n",
    "\n",
    "table = ax8.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                  colWidths=[0.6, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style the header row\n",
    "for i in range(2):\n",
    "    table[(0, i)].set_facecolor('#34495e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(table_data)):\n",
    "    for j in range(2):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#ecf0f1')\n",
    "        else:\n",
    "            table[(i, j)].set_facecolor('white')\n",
    "\n",
    "ax8.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.suptitle('Training Time & Compute Usage Analysis: Normal vs Loss-Based Batch Selection', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig('training_efficiency_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING EFFICIENCY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'METRIC':<35} {'NORMAL':<15} {'LOSS-BASED':<15} {'IMPROVEMENT':<15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Total Training Time (s)':<35} {time_normal:<15.2f} {time_loss_based:<15.2f} {time_saved_percent:>14.1f}%\")\n",
    "print(f\"{'Total Samples Processed':<35} {total_samples_normal:<15,} {total_samples_loss_based:<15,} {compute_saved_percent:>14.1f}%\")\n",
    "print(f\"{'Final Test Accuracy':<35} {results_normal['test_acc'][-1]:<15.4f} {results_loss_based['test_acc'][-1]:<15.4f} {(results_loss_based['test_acc'][-1] - results_normal['test_acc'][-1]):>+15.4f}\")\n",
    "print(f\"{'Speedup Factor':<35} {'1.00x':<15} {f'{speedup:.2f}x':<15} {'':<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EPOCH-BY-EPOCH BREAKDOWN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'EPOCH':<10} {'NORMAL':<20} {'LOSS-BASED':<20} {'SAMPLES SAVED':<20}\")\n",
    "print(\"-\"*70)\n",
    "for i in range(EPOCHS):\n",
    "    samples_saved = results_normal['samples_per_epoch'][i] - results_loss_based['samples_per_epoch'][i]\n",
    "    print(f\"{i+1:<10} {results_normal['samples_per_epoch'][i]:<20,} \"\n",
    "          f\"{results_loss_based['samples_per_epoch'][i]:<20,} \"\n",
    "          f\"{samples_saved:<20,} ({samples_saved/results_normal['samples_per_epoch'][i]*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. (Optional) Test on Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both models on a few examples\n",
    "def predict_sentiment(model, text, tokenizer):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids = encoding['input_ids'].to(DEVICE)\n",
    "        attention_mask = encoding['attention_mask'].to(DEVICE)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        prediction = outputs.logits.argmax(dim=1).item()\n",
    "    \n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Terrible film. Complete waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\"\n",
    "]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"=\"*60)\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    pred_normal = predict_sentiment(model_normal, text, tokenizer)\n",
    "    pred_loss = predict_sentiment(model_loss_based, text, tokenizer)\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Text: {text}\")\n",
    "    print(f\"  Normal Model: {pred_normal}\")\n",
    "    print(f\"  Loss-Based Model: {pred_loss}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

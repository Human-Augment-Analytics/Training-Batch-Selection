# Quick Reference Guide

## Vision Experiments - Quick Commands

### ğŸ¯ Recommended: Single Command Pipeline

```bash
# Run complete pipeline (experiments + comparisons + benchmark)
python -m tasks.vision.run_all

# Quick test mode (1 epoch, 1 run) - perfect for testing changes
python -m tasks.vision.run_all --quick

# Run on specific dataset
python -m tasks.vision.run_all --dataset cifar10_csv

# Skip benchmark if you only want experiments + comparisons
python -m tasks.vision.run_all --no-benchmark
```

**What does `run_all` do?**
1. âœ… Runs experiments with ALL batch strategies (Random, Fixed, Smart)
2. âœ… Generates comparison plots between strategy pairs
3. âœ… Benchmarks one strategy across multiple datasets
4. âœ… Saves all results to organized output folders

---

## Before vs After

### âŒ Old Way (3 separate commands)
```bash
# Step 1: Run experiments
python -m tasks.vision.run_experiment --dataset mnist_csv

# Step 2: Generate comparisons
python -m tasks.vision.compare --dataset mnist_csv

# Step 3: Benchmark
python -m tasks.vision.benchmark --strategy Random --datasets mnist_csv qmnist_csv
```

### âœ… New Way (1 command)
```bash
python -m tasks.vision.run_all
```

---

## Individual Commands (Still Available)

If you need more control, individual commands still work:

```bash
# Run experiments only
python -m tasks.vision.run_experiment --dataset mnist_csv

# Compare strategies only
python -m tasks.vision.compare --dataset mnist_csv

# Benchmark only
python -m tasks.vision.benchmark --strategy Random --datasets mnist_csv qmnist_csv
```

---

## Common Workflows

### Testing Code Changes
```bash
# Quick test with 1 epoch, 1 run
python -m tasks.vision.run_all --quick
```

### Full Production Run
```bash
# Edit config/vision.py first to set:
# EPOCHS = 10
# N_RUNS = 5

# Then run
python -m tasks.vision.run_all
```

### Experiment on New Dataset
```bash
# Run on CIFAR-10
python -m tasks.vision.run_all --dataset cifar10_csv

# Run on CIFAR-100
python -m tasks.vision.run_all --dataset cifar100_csv
```

### Skip Time-Consuming Benchmark
```bash
# Only run experiments and comparisons (faster)
python -m tasks.vision.run_all --no-benchmark
```

---

## Output Structure

After running `python -m tasks.vision.run_all`, you'll get:

```
outputs/vision/
â”œâ”€â”€ mnist_csv/                          # Your primary dataset
â”‚   â”œâ”€â”€ batching_random/
â”‚   â”‚   â””â”€â”€ run-001/
â”‚   â”‚       â”œâ”€â”€ test_acc.png
â”‚   â”‚       â”œâ”€â”€ train_acc.png
â”‚   â”‚       â”œâ”€â”€ train_loss.png
â”‚   â”‚       â”œâ”€â”€ test_loss.png
â”‚   â”‚       â””â”€â”€ summary.txt
â”‚   â”œâ”€â”€ batching_fixed/
â”‚   â”‚   â””â”€â”€ run-001/
â”‚   â”œâ”€â”€ batching_smart/
â”‚   â”‚   â””â”€â”€ run-001/
â”‚   â”œâ”€â”€ comparison_random_fixed/        # Comparison plots
â”‚   â”‚   â”œâ”€â”€ test_acc_cmp.png
â”‚   â”‚   â”œâ”€â”€ train_acc_cmp.png
â”‚   â”‚   â””â”€â”€ train_loss_cmp.png
â”‚   â””â”€â”€ comparison_random_smart/
â”‚
â””â”€â”€ benchmarks/                          # Benchmark results
    â””â”€â”€ random_multi_dataset/
        â”œâ”€â”€ summary.txt
        â”œâ”€â”€ mnist_csv_test_acc.png
        â”œâ”€â”€ qmnist_csv_test_acc.png
        â””â”€â”€ ...
```

---

## Configuration

Edit `config/vision.py` to customize:

```python
# Training parameters
EPOCHS = 5              # Number of epochs per run
N_RUNS = 5              # Number of runs for statistics
BATCH_SIZE = 64         # Batch size

# Default dataset
ACTIVE_DATASET = "mnist_csv"

# Model architecture
HIDDEN_DIM = 256
```

Edit `config/batch_strategies.py` to configure comparisons:

```python
# Which strategy pairs to compare
VISION_STRATEGY_COMPARISON_PAIRS = [
    ("Random", "Fixed"),
    ("Random", "Smart"),
]
```

---

## Tips

- **First time?** Use `--quick` to verify everything works
- **Debugging?** Individual commands give more control
- **Production?** Use `run_all` without flags for complete analysis
- **Time-constrained?** Use `--no-benchmark` to save time
- **GPU not detected?** Run `python scripts/check_device.py` to diagnose